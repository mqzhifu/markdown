# 监控/调优指令

优化实体：CPU 内存 硬盘 网卡

步骤：

```mermaid

graph LR

概览-->部分概览

部分概览-->详细分析

详细分析-->锁定进程

```

  

```mermaid

graph LR

uptime/vmstat-->sar/top

sar/top-->free/df/iostat/iptraf-ng

free/df/iostat/iptraf-ng-->tcdpdump/netstat/trace

```

常用指令：

| 指令      | 描述                        | 目标            | 分类   |
| ------- | ------------------------- | ------------- | ---- |
| uptime  | 采样计算5/10/15\(分钟\)内CPU的负载度 | cpu/进程        | 汇总   |
| vmstat  | 汇总信息                      | mem/cpu/io/进程 | 汇总   |
| sar     | 以时间维度汇总CPU/网络等数据          | 所有            | 中等汇总 |
| top     | 查看计算机整体信息,主要是CPU          | CPU/进程/内存     | 中等汇总 |
| free    | 内存使用的情况                   | 内存            | 详细   |
| iostat  | 硬盘IO数据                    | 硬盘            | 详细   |
| df      | 查看硬盘挂载                    | 硬盘            | 详细   |
| du      | 查看目录/文件大小                 | 硬盘            | 详细   |
| strace  | 进程追踪                      | 进程            |      |
| iptraf  | 网卡流量统计                    | 网卡            |      |
| netstat | 网络连接统计                    | 网卡/进程         |      |
| tcpdump | 网络连接统计                    | 网卡/进程         |      |
|         |                           |               |      |

## 文件汇总，使用cat

| 文件              | 描述     |
| --------------- | ------ |
| /proc/meminfo   |        |
| /proc/diskstats |        |
| /proc/loadavg   |        |
| /proc/stat      |        |
| /proc/net/dev   |        |
| /proc/vmstat    |        |
| /proc/loadavg   | uptime |
| /proc/net/dev   |        |
|                 |        |

> 实际上所有指令调取的数据均从这些文件中取出的

## uptime

> 这个有点复杂，我也只能看懂一点简单的理解

1/5/15\(分钟\)内进程队列中平均进程数，包括正在运行的进程\+准备好等待运行的进程。

如果进程队列中有需要运行的进程，那就证明其实已经阻塞了，因为如果CPU执行的速度够快，这里的值应该经常是空的才对。 也就是值为：0 0 0

但虽然阻塞，如果执行时间够快，能够满足一个CPU的正常执行时间，其实也还好，也就是 1 1 1 其实也无所谓，CPU满载，也没超速计算。

而如果大于1 ，也就是队列里等待需要运行的进程过多的时候，就有点问题了， 2 3 4

由此得知：1可以做为基准值，如果到达0.8左右 ，你就得看一看了，如果等于1那真得好好看了，如果超过1就必须得看看了。

然而，进程队列，每一个CPU都会有一个，那么，以1为基准值也就是不准的，应该以CPU核数为基准值，即：当前值 \>= 核数 即危险了。

## top

宏观看一下当前计算机的大概负载，使用的频繁挺高。它分两个部分，

1. 汇总信息
    \(1\) 开机总时长 当前登陆用户总数 5/10/15\-load值
    \(2\) 总运行进程数 正在运行中数 睡眠中数 已停止数 僵尸进程数
    \(3\) CPU用户态总使用率 系统态 调用 空闲 等待 硬件中断 软件中断 实时
    \(4\) 总内容数 已使用 空闲 buff/cache
    \(5\) swap
2. 进程列表

|关键字 |说明                                                               |
|-------|-------------------------------------------------------------------|
|PID    |进程ID                                                             |
|USER   |启动进程的用户名                                                   |
|PR     |优先级                                                             |
|NI     |nice值。负值表示高优先级，正值表示低优先级                         |
|VIRT   |进程使用的虚拟内存总量，单位kb。VIRT=SWAP\+RES                     |
|RES    |进程使用的、未被换出的物理内存大小，单位kb。RES=CODE\+DATA         |
|SHR    |共享内存大小，单位kb                                               |
|S      |进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程|
|%CPU   |占用的 CPU 使用率                                                  |
|%MEM   |占用的记忆体使用率                                                 |
|TIME\+ |进程使用的CPU时间总计，单位1/100秒                                 |
|COMMAND|进程的启动指令                                                     |

> 其实还有些字段没列出来，上面是主要的，另外主要的这些也不经常用，经常用的就是：CPU占有率、内存占有率，进程IP。

## free

free \-m :以MB为单位，展示当前内存汇总情况

|关键字    |说明                                                                 |
|----------|---------------------------------------------------------------------|
|total     |总内存数                                                             |
|used      |已使用内存数                                                         |
|free      |未使用内存数                                                         |
|shared    |多个进程共享的内存总和，当前废弃不用                                 |
|buff/cache|缓冲/ 缓存数                                                         |
|available |还可以被应用程序使用的物理内存大小，available=free \+ buffer \+ cache|

实际他是分成两行的：mem和swap

mem:就是正常的物理内存

swap:使用了硬盘内存交换模式，一但这行有值，程序真该优化了，不过大部分时间此值是空的

buffers：存inode,IO写

cache ：存page cache，IO读。

## 内存计算

buffers/cache 两个东西，都是给CPU加速的，缓存了常用的\<系统操作\>信息。如：频繁IO的文件FD、内存块等，OS会缓存住。\(属于系统控制\)

由上边，结合看，因为buffer/cache和share这两值的存在，单看free字段意义不大，因为这个值会一直很小，同时慢慢的buffer/cache的值会越来越大。最后，buffer/cache会把余下的内存都给占住\(95%左右\)，但后续如果继续开新进程，内存不够，OS会释放buffer/cache这部分内存。

总结下就是：free不准，buffer/cache会动态变化，得动态结合计算。

内存剩余计算，可以直接使用available字段

但这种换个角度看也有问题，因为buffer / cache 实际已经分配了内存，只是内存不够时，再开新进程才会释放

内存已使用率，可直接使用used，但这又忽略了buffer / cache 已分配的内存

## CPU计算

cpu占用比率分类：

|关键字        |描述                           |
|--------------|-------------------------------|
|user          |用户态时间                     |
|system        |内核态时间                     |
|nice          |用户态时间\(低优先级，nice\>0\)|
|idle          |空闲时间                       |
|wait          |I/O等待时间                    |
|hard interrupt|硬件中断                       |
|sort interrupt|软件中断                       |
|steal time    |实时                           |

> wait时间是不可靠值

jiffies:从开机到现在，CPU总中断次数。而多久能中断一次，取决于hz单位。hz越高中断的次数越多，hz的时间单位可以以秒计算，如：200m/hz ，每秒200M\-HZ，1M=1000 000HZ，200M\-HZ=1秒/200 000 000HZ=0.000 000 005 ,也就是每0.000 000 005秒中断一次。5纳秒中断一次。

正常的时间单位换算：1秒=1000毫秒=1000 000 000纳秒

然后，定时中断，也就决定了，在中断期间可以对时间累加，换个角度说：这个周期就决定了LINUX系统时间及精准度。间接等于: 时间的计算与管理

> 蛋疼的我，去查了下自己电脑的CPU\-HZ，居然是以G为单位...靠，也就是说准确度大于纳秒级别...

虽然物理CPU可以支持如此高的精度，但是好像OS层面是可以动态设置这个值的，取决于内核配置文件，好像不太高呢... \(可能是为了性能，也可能是牵扯到溢出吧\)

> vim /boot/config\-3.10.0\-327.el7.x86\_64

搜索：CONFIG\_HZ=

结果是：1000，也就是一个jiffies = 1毫秒

> 2.4以前的linux kernel 都是100，也就是10毫秒，目前只查到1跟10没查到其它单位。

内核可能可能会丢失的时间中断？

既然精准度是毫秒，那LINUX是如何模拟出微秒的呢？

> 除了上面的1000，LINUX还会另起一个以纳秒为单位的计时器，然后，通过wall\_jiffies去更新xtime，有意思的是这个这个节奏器只是无限接近于一毫秒，有误差

以上只是说了一下CPU占比时长，接下来说一下CPU占比分类的详细含义

用户态时间:过长，这个是比较正常见的，因为大部分都是以用户态启动一个项目，然后在用户态执行代码

系统态时间:过长，这个比较复杂，进程死锁造成频繁上下文切换\(调度\)，内存频繁page复制\(内存不够\)，硬盘频繁IO，基本上这个值高就危险了...

IO等待时间:过长，这个就得查看硬盘了

## iostat

device:硬盘设备ID号

tps：该设备每秒的传输次数,多个逻辑请求可能会被合并为"一次I/O请求"

Blk\_read/s:每秒读数据量

Blk\_wrtn/s：每秒写数据量

Blk\_read：总读数据量

Blk\_wrtn:总写数据量

## vmstat

一个概览性的报表：mem/cpu/io/进程

vmstat 2 1 \-S m

主要有两个参数，第一个参数：死循环，几秒一次~ 第二个参数：执行几次,\-S m:以MB单位统计

\[root@localhost ~\]\# vmstat 3

procs \-\-\-\-\-\-\-\-\-\-\-memory\-\-\-\-\-\-\-\-\-\- \-\-\-swap\-\- \-\-\-\-\-io\-\-\-\- \-\-system\-\- \-\-\-\-\-cpu\-\-\-\-\-

r b swpd free buff cache si so bi bo in cs us sy id wa st

1 0 11228 63332 190620 444176 0 0 0 3 1 1 0 0 99 0 0

r 表示运行队列\(就是说多少个进程真的分配到CPU\)，当这个值超过了CPU数目，就会出现CPU瓶颈了。一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。

b 表示阻塞的进程

swpd 现时可用的交换内存,虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。

free 空闲的物理内存的大小

buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M

cache 高速缓存,直接用来记忆我们打开的文件,给文件做缓冲，\(inux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，提高性能，当程序使用内存时，buffer/cached会很快地被使用。\)

si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。

so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。

bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据\(2\-3T\)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒

bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。

in 每秒CPU的中断次数，包括时间中断

cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者，压线程的峰值一直下调测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。

us 用户进程使用的时间，百分比计算，用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80\(机器在做压力测试，性能表现不佳\)。

sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。

id 空闲 CPU时间，一般来说，id \+ us \+ sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。

wt 等待IO CPU时间。

## 网络

3方软件：

|软件名|描述                                                               |
|------|-------------------------------------------------------------------|
|iftop |比较小巧，界面简单，没iptraf强大，但是基础的功能有，收发包 平均速度|
|ntop  |安装略复杂，带WEB\-UI，放弃了                                      |
|iptraf|一个可视化UI界面的软件                                             |
|nload |跟iftop有点像，支持实时，但是显示数据还没iftop多                   |
|ifstat|最小巧，只是输出几个网卡平均值...跟日常ls mkdir 有点像             |

系统自带

sar –n DEV 1 4

ifconfig eth0 rx=接收 tx=发送

总结

系统自带的：要么还得二次计算，要么比较简单 ，3方的 iptraf 和 iftop 略好点，决定用iptraf吧

iptraf的功能

1. 可查看全部/单个网卡：汇总信息\(iptraf\-ng \-d eth0\)
2. 可查看全部/单个网卡：详细信息，如当前哪个IP:PORT连接了哪个IP:PORT的流星统计\(iptraf \-i \-d eth0\)
3. 支持动态变更（数据不停刷新）
4. 将统计信息输出到文件中\(iptraf \-B\)
5. 可设置filter ，只监听某一个IP:PORT的数据\(filter\-\>IP\-\>define new filter\)

缺点

1. 这个UI有点别扭，有乱码情况，得设置vt100模式修复
2. 相比tcpdump 还是少几个field

## netstat

很实用/常用的指令，统计网络连接的相关情况.

\-a \(all\)显示所有选项，默认不显示LISTEN相关

\-t \(tcp\)仅显示tcp相关选项

\-u \(udp\)仅显示udp相关选项

\-n 拒绝显示别名，能显示数字的全部转化成数字。

\-l 仅列出有在 Listen \(监听\) 的服務状态

\-p 显示建立相关链接的程序名

\-r 显示路由信息，路由表

\-e 显示扩展信息，例如uid等

\-s 按各个协议进行统计

\-c 每隔一个固定时间，执行该netstat命令。

> 分析看，\-a \-n \-t 参数是基本上都得加上

统计当前80端口的总连接数

> netstat \-nat|grep \-i "80"|wc \-l

统计 TCP状态为:TIME\_WAIT的总数

> netstat \-nat |grep TIME\_WAIT | wc \-l

当前进程已监听的端口号的进程列表

> netstat \-nlt

## 总结

以互联网开发来看：

1. DB、REDIS、图片/文件 都在云上\(磁盘IO、大数据量排序分组联表计算吃CPU、缓存吃内存等都在吃云端资源\)
2. 辅助软件\(es consul prometheus\)都在其它机器上
3. 网络连接用的都是开源软件\(开源软件的性能及对临界值的处理非常好，减少了吃CPU 内存 IO的情况\)

只要一台机器上不启动过多的自研应用进程\(自研的东西鬼知道有多少问题\)

服务器能出现问题的地方就一个：网络，出现的原因

1. 访问量过多，造成TCP连接不够
2. 等待后端的连接过长，造成了阻塞
3. 传输内容略大，造成带宽不够

分析下来，互联网的开发的更倾向：IO型运算，且再加上一些分布式做负载，没太大技术含量....
