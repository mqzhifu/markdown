# Trie/前缀树/字典树


![[前缀树.png]]

操作字符串的一种算法~

1. 给定若干单词\(字符串\)，构建树
2. 个节点只有一个字母
3. 把单词拆成一个一个的字母，然后插入到树中即可

> 这是个多叉树

用途：

1. 前缀匹配，搜索框里的模糊匹配，如：输入go 提示出：google golang 等
> 换种方式，也可以直接到MYSQL中用sql模糊查找，也可以从数组里逐一遍历，但字典树明显效率更高一点
2. 词频统计：一本书中出现次数最多的单词
3. 判断某一个字符串是否出现过
4. 去重，给出少量的单词，找出只出现过一次的单词

优点：

1. 比数组/链表/hashmap 性能要高一点
2. 比数组/链表/hashmap 更节省空间

缺点：


1. 复杂度略高
2. 得先构建树


既然有前缀，当然也可以反过来，做后缀树，但感觉没啥实际场景。

# hash-tree / Merkle Tree

![[merkle_tree.png]]

1. 一个大文件，被打散成10个小文件
2. 读取 10个小文件的内容，用 hash 函数加密码得到10个hash值
3. 将此10个hash 每2个为一组，再进行一轮HASH，得到5个新的hash值
4. 重复上面操作，递归，继续合并
5. 直到最后只剩下一个hash值：根 hash

当有一个大文件：
1. 判断其是否修改
只需要判断根的hash值即可
1. 如果文件修改，确定是哪个文件块做了修改
从根节点开始查找，类似二叉树

>这样，即使一个非常大的文件，发生改变，也不需要遍历整个文件，就能算出是否被修改以及哪里做了修改

什么场景使用？

1. 文件下载
2. 数据一致性
3. 比特币的所有交易记录

# 外部存储\-排序

理解状态下：把所有数据从硬盘中读取到内存中计算(排序)
现实状态下：数据量过大，内存不够，就得借用硬盘来做外部排序
>大部分的排序都得借用硬盘IO

核心思想：归并排序

假设，当前内存只够1000条数据 ，而此时的总数量为10000

1. 按照顺序，每次读取1000条，然后排序，将1000条有序的数据存于某个特定文件中，也可以存于某个大文件中
2. 这一轮操作后，10000条记录被分成10个段，每个段都是有序了，也就是：10000条记录局部有序
3. 第一段中读取500，再从第二段中读取500条，然后排序，存于一个文件中
4. 第一段中读取剩下的500条，再从第二段中读取剩的500条，然后排序，存于一个文件中
5. 经常3和4步骤原本10段的数据被合并成了5段，不停的重复3 和4步骤，直到排序完成

多路归并排序

上面就是两个文件进行比对，硬盘IO有点高，如果此时，进行多路归并，比如把2个合并，改成3个一起合并，效率能略好一点

# Huffman 编译压缩

![[哈夫曼.png]]

> 树结构：并不一定局限于对数字的\<加快搜索\>操作，它也可以用做：表达式解析、编码压缩等。

假设有一个字符串： SUSIE SAYS IT IS EASY

1. 统计每个字符出现的次数，即：频数

| 字符  | 出现次数 | 编码    |
| --- | ---- | ----- |
| A   | 2    | 010   |
| E   | 2    | 1111  |
| I   | 3    | 110   |
| S   | 6    | 10    |
| T   | 1    | 0110  |
| U   | 1    | 01111 |
| Y   | 2    | 1110  |
| 空格  | 4    | 00    |

> 这里其实还没有生成：编码，只是暂时先列出来

1. 排序，根据每个字符的出现次数
2. 将排序好的字符数组压入栈中\(栈顶是频数最小的\)，如下：

> 关于相等值的压入顺序可以忽略，并不影响

| 字符  | 出现次数 | 代替编码  |
| --- | ---- | ----- |
| 换行  | 1    | 01110 |
| U   | 1    | 01111 |
| T   | 1    | 0110  |
| Y   | 2    | 1110  |
| E   | 2    | 1111  |
| A   | 2    | 01111 |
| I   | 3    | 110   |
| 空格  | 4    | 00    |
| S   | 6    | 10    |

>这里看出：出现次数越少，编号越长，出现次数越多，编号越短


> S出现次数最多在栈底，最少的在栈底

1. 结合栈，构建Huffman树
    1. 每次从栈中弹出两个节点
    2. 创建一个新的：树节点
    3. 将两个栈节点挂在新的树节点的两边
    4. 然后，将新节点重新压入到栈中
    5. 新节点重新压栈得有规则，也就是比对计算位置，也就是栈里的节点是有序的\(根据出现次数\)
    6. 栈节点的关键值是：出现次数，新节点是两个原节点的次数之和
2. 根据已构建的Huffman树，开始连线，生成每个字符的二进制编码\(表格中已提前生成\)
3. 根据编码表格，开始压缩字符串，如下：

> 10 01111 10 110 1111 00 10 010 1110 10 00 110 0110 00 110 10 00 1111 010 10 1110 01110
> 
> 
> 注：实际情况没有空格，只是便于理解才加上的空格

1. 解码，把给定的字符串，每一位，拿到构建的Huffman树，按照路径，0向左走，1向右走，只要碰到是一个字符，即输出，同时回到Huffman树根部，重新再执行此步骤。

此算法如何做到了压缩功能？

> 正常字符流，任何一个字符都占1个字节，即：8个二进制位，Huffman算法，把出现频率最高的字符，由8个位换成了2个位，实现了压缩。

核心算法：通过一个容器\(有序栈\)，构建一个树，通过树再构建一个字符替换编码，再通过树进行压缩/解压缩

优点：加速了传输的速度。

缺点：

1. 肯定得牺牲些CPU的计算时间，用于压缩和解压缩，但以目前的计算机体系，可以忽略。
2. 从算法上看，被压缩的文档，重复的值越多，它的算法肯定越好，反之，压缩的效果一般。

## 总结

树结构其实算是对 线性结构\(数组、链表\) 的优化，比较适合 读多写少且重复值少的情况，像给一个二维表中的某个字段做索引。

