# 概要

同步，即是将一条数据发送给另一个(多个)人
本质上，程序员都写过类似的代码。如：
浏览器的 HTTP 协议，B 端发个请求，S 端将数据整理好返回给 B 端

> 只是根据代码完成的不同功能，有不同的差别。

## 分类

1. 传统型的 APP，如：旅游、教育、天气、外卖类等
2. 游戏的 APP

### 传统偏应用型 APP

更像是针对某一个人，服务端只要把数据给我即可

### 游戏

> 针对的是很多个人，同时要把数据推送给游戏多个玩家，并且，要讲究实时性，同时会有大量的渲染工作

## 关键点

1. 同步给多个人
2. 实时性
3. 要把数据做渲染~转换成动画/视频

游戏的网络同步=实时的多端数据同步+实时多端表现同步

## 从硬件上看两者差别

### 游戏

IO 网络性能\+CPU 密集型运算\+GPU 密集型运算
但数据可小范围丢失，没非常高的要求，不太讲究持久化

### 传统型 APP

网络 IO\+硬盘 IO
对并发、数据存储有硬性要求，数据不允许丢

## 移动网络

### 手机接/打电话原理

基站与基站的通信，而不是手机与手机直接通信。手机有一套自己的通信机制/协议，现在硬要把 PC 网络通信的东西加进来，且要兼容，所以，就只能在原协议构架上再伪造出一层 IP 协议通信。

基站的缺点

1. 手机的基站内容就是超复杂的系统，基站好像还要再转到内部核心网络。网络里面好像还有什么 WAP 网关（政府控制）、鉴权、计费、跳转
2. 一个基站可承受最大连接数是有限制的，一但人多，又完犊子了。
3. 就算人多能连接上，那么，信道的频繁调度又会变慢。
4. 基站也不可能像路由器一样，100 米内就有，肯定距离远

结论：不稳定 。其实总结下来就是：手机的通信核心是声音的传递，声音是可以丢失的，且断断续续的也行，丢点数据无所谓。但现在非要在此协议上模拟出个 IP 层.....但 4G，尤其 5G 的大跃进真的是一件牛 B 的事情...

运营商、通信协议号太多，如：

1. 中国移动
2. 中国联通
3. 中国电信

每个大的分类下放还有：4G、5G、2G

协议制式：GPRS、EDGE、TD-SCDMA、TD-LTE

另外，还有中国特色的：

1. 联通的不能访问移动的.....
2. 运营商的内部缓存/劫持

### 问题

1. 高延迟、易抖动丢包、通道狭窄
2. 接入类型、接入点变化频繁

开始步入正题，游戏的数据同步

# 数据同步分类

1. 帧同步
2. 状态同步
3. 混合同步

## 简易对比

帧同步：

- 对网络、延迟要求较高
- 复杂度低 k
- 可较好抽象封装，利用率更高一点
- 防外挂较弱
  > 如：RTS、格斗类，对实时性较高的游戏，比较适合，因为传输的内容较少

状态同步

- 对网络、延迟要求并不是特别高
- 复杂度极高
- 防外挂较好

混合同步

帧同步+状态同步，具体看业务场景，可能对实时较高的场景用帧同步，对数据安全性较高的使用状态同步，两者配合使用

这里只作帧同步描述，另外两种不做处理

# 帧同步实现原理

将每个玩家的操作指令，发送到 S 端，S 端接收后，再统一发送(广播)给所有玩家，玩家的客户端接收到操作指令，分析指令该执行什么样的动作、UI 如何产生变化 ，最后开始在本地渲染生成动作效果，最后，计算一下碰撞的结果。

大体流程，如下图：[[帧同步-简版.png]]

玩家 1 产生指令 A，玩家 2 产生指令 B，各自将指令 A 和 B，发送到 S 端，S 端收到 AB 玩家指令，然后将 AB 打包合成一个逻辑帧叫：C，广播给玩家 1 和 2，玩家 1 和 2 都会接收到该逻辑帧，然后开始各种的本地渲染。

代入场景中：玩家客户端 A，在匹配成功后，会拿到玩家 B 的信息，然后初始化两个玩家及游戏场景信息。开始游戏，会有一些按钮/轮盘，客户端收集玩家操作（按钮/轮播）的指令，发送 S 端，S 端接收打包成一个集合，在一个时间片内，返回给客户端指令集合，客户端开始模拟指令，在本机计算并渲染。

## 关键步骤

1. 时间片
2. 客户端发送玩家操作指令
3. S 端收集客户端发送的指令并打包成逻辑帧
4. S 端推送逻辑给客户端
5. 客户端在本地沉浸

## 简单分析

我们看到，图里前半部分，1 和 2 玩家各自发送了：一个操作帧，而后半部分，玩家 1 在一个时间片内发送了 2 个操作帧，而玩家 2 只发送了一帧。

所以，核心点就是时间片的定义。

另外，这里还牵扯到一些问题

1. 客户端发送操作帧，丢包
2. 服务端接收操作帧，丢包
3. 客户端接收逻辑帧，丢包
4. 服务端发送逻辑帧，丢包
5. 玩家网速有快有慢，如果采取等的模式\(囚徒模式\)，即，一个时间片内必须收到所有玩家的操作帧，那么，网速最快的玩家就比较亏，游戏体验也差，因为完全依赖网速最慢的那个玩家。
6. 接上，网速快的玩家可以在一个时间片内有多个操作~而网络慢的玩家，在一个时间片内，发的消息，可能只成功了一条，即：只有一个操作帧生效了。那这样网速慢的也吃亏。

帧同步也有几个版本，先简单看一下。

## Lockstep

---

锁步同步、一致同步\(囚徒模式\)

每个玩家的操作指令，发送到 S 端，S 端接收到，分发给所有玩家，等待所有玩家发送已确认指令后，再接收下一条指令，继续分发，这个就是我上面的图

### 优点

代码实现起来是最简单的，因为是每一帧都要所有人确认。另外，尽量做到公平~

### 缺点

游戏的流畅度取决于网络最慢的那个

### 总结

对网络性能要：RTT 100ms 以内，一般人数不超过 8，流畅度得以提升，并且，公平性较高。比较有代表性的就是：魔兽、星际等，一但某个人网络很差、掉线，其它 7 个人就会弹出窗口一起等待。

## 乐观锁步

> 算是对囚徒模式的一个小升级

C 端依然是发指令即发送到 S 端，但是 S 端并不立刻广播消息，而是将一秒钟平均分成 10 个段，也就是 100MS 为一个段，在这个时间段内，将收集好的指令统一广告给所有人~也不需要 C 端返回 ack 了

优点：解决了：一个人网慢，带着所有人都慢，统一设定一个延迟时间，最大限度容忍延迟

缺点：

- 频率设置，是以所有玩家中最低的网速来设定还是以最高的设定，又或者取中间值 ，又或者是动态控制的
- 因为是人为设定了一个延迟率，对实时性超高的游戏不适合
- 增加了编程复杂度，如：某个玩家网络抖动后，瞬间会接收很多的帧，得顺序渲染，但同时还有新的逻辑帧同步过来，得有策略处理，另外可能还有动态频率

## 回滚/预测/快速 predict rool\-back snapshot

预测：当玩家操作后，C 端接收到指令后，立刻做出响应，不等 S 端发送过来的逻辑帧。这里不光预测自己，还测试对手。

回滚：接上，接收 S 端延迟回来的逻辑帧后比对，一但发现自己的预测不对，即立刻回滚到正常逻辑帧号上。

C 端：延迟补偿、插值、dead reckoning（导航推测）

这个先不写了，略复杂

# 网络传输协议的选择

## TCP

1. 时序性
2. 重传机制
3. 应答机制
4. 数据延时：粘包
5. 自动拆包

优点：面向连接、拥塞控制、流控、小包合并、ACK 确认机制、滑动窗口等等，总之就是：可靠

缺点：慢，因为上面的一堆算法，导致必然会慢，如果对实时性要求不高的，像一些小游戏、卡牌类的应该够用，但是 RTS 类的，10 帧以内可以，10 帧以上延迟、卡顿还是挺明显的。

## UDP

特性：

1. 无连接
2. 一对一、一对多、多对多\(单播、广播、多播\)
3. 对应用层数据直接打包
4. 尽最大努力交付，也就是不可靠，不使用流控

优点：快....头部仅 8 个字段，非面向连接，也没什么拥塞、流控之类的，且是单程传输，延迟非常低~两端都可以把焦点聚集在业务上。

缺点：不可靠，丢包~得自己再封装一层协议，保证\<大概率\>可靠性

## 具体分析

这里其实可以写很多条，但感觉都很鸡肋，其实二条就够了，如下：

1. 开发周期。使用 UDP 虽然快，但是得自己封装一层类似 TCP 的一层，要有 流控、不丢包等等，开发成本太高，几何倍增长。基本上都是有一定用户量后，且是大公司，才可能上 UDP。并且，还得有阵痛期，必然自研的东西，得经过时间考证
2. 对开发要求高。就算你有时间成本做，也得有相关的高级程序员来写，而且得是比较资深的网络编程类型的人才，不然 BUG 一堆，还不如用 TCP 呢....

## 不要迷信 UDP

1. UDP 没有粘包的情况，但也是缺点。每包必发，如果通信数据都很小，每包也必发，那势必会占用带宽。
2. 如果每次通信的数据非常在，IP 层会做分片，一但有一片丢失，整个数据既会被丢掉。

总结：UDP 需要自己实现：确认机制、流量控制、拥塞控制，控制不好，造成风暴模式，狂占用带宽，挤压其它业务

## 消耗对比

UDP 比 TCP 浪费 10%-20%的带宽的代价，换取平均延迟降低 30%-40%，且最大延迟降低三倍的传输效果
综合上面，UDP 通过各种损失+开发成本 换取更低的延迟率

## UDP 具体实现方法

1. Reliable UDP 稳定性 UDP，可参考 KCP UDT
2. 冗余 UDP，IO 时，除了本帧内容再附加上一帧或者下一帧数据

这里不做详细解说

# 传输数据类型协议分析

## json

优点

1. 这个最简单，现成的，没有开发成本，也没什么维护成本
2. 可视化最好，因为是字符流，且是对象传输，很方便调试
3. 扩展起来也相对简单，两端也不需要加什么类库

缺点
慢！慢！慢！

## protobuf

缺点

1. 二进制流传输，可视化不好，不方便调试
2. 开发成本高，两端需要学习 protobuf 协议，两端代码层还得增加类库
3. 开发不友好，每次改个 key，两端还得重新编译原文件，再放到特定位置
4. 具说，C\# U3D 每次解/压 protobuf 会有 GC 问题

## 自研协议

缺点：

1. 二进制流传输，可视化不好，不方便调试
2. 开发成本非常高，除了不能可视化，还要学习位移，且调试的时候，要一个位一位的去对
3. 扩展起来也是头疼

优点

1. 最快
2. 遇到底层不好处理的问题，这里依然可以非常好的方式处理
3. 定制化更强

## 小结

大多都是 protobuf ，够用。中小公司完全够用，大公司估计得走自研了，protobuf 在内存开销挺大的。

# 具体实现

## 实现目标

1. 时间尽量短些，尽快出 DEMO
2. 保证能够正常的同步，能让游戏动起来
3. 至少保证 FPS 10 帧
4. 流畅性~至少不能总卡吧
5. 支持多人同人游戏，怎么着也得 4 人吧...
6. 文档尽量要全
7. 接口定义尽量大众化
8. 兼容 COCOS 引擎（网页版）
9. 软件尽量写的兼容性强一些，可扩展性也要考虑
10. 至少保证 lockstep 模式
11. 虽然只是要实现帧同步功能，但还是要考虑下：匹配服务、房间服务的
12. 最好能单独封装成一个微服务，更加专业化

## 问题思考

1. C 端每执行多少条指令推送一次 S 端？不要说每一个操作都同步，太奢侈了...

   > 如果采用定时发指令，这个时间多久是最优的？
   > 如果采缓存发指令，这个多少条指令最优的？

1. S 端接收多少条 C 端的指令集广播一次？不要说每一个操作都同步，太奢侈了...

   > 如果采用定时发指令，这个时间多久是最优的？
   > 如果采缓存发指令，这个多少条指令最优的？

1. C 端接收到指令后，渲染的过程会不会卡顿？比如：

   > 某一帧的指令特别多
   > 某一帧的某条指令渲染时特别耗时
   > 玩家掉线后，接收期间的所有指令，录放的时候渲染肯定会卡

1. S 端广播的包体大小限制为多少才是最好？MTU
1. C 端发送的数据包大小限制多少才是正好？
1. 浮点数受 OS 及硬件影响，传输是不一样的？确定性碰撞库 PEMatch

   > 如：GPS 位置，传输的时候要考虑丢失，最好是转成 INT

1. 传输协议？TCP/UDP
1. 传输数据类型？JSON/PROTOBUF
1. S 端要支持一局游戏，可以有玩家任意时间加入的场景
1. 是否需要定义关键帧？
1. C 端渲染的时候，是否可以异步处理？而非顺序阻塞模式？
1. 概率性的值如何保证一致性？如：暴击率、装备掉落
1. C 端，多线程/协程，如何保证时序？
1. TCP 粘包？

## 术词解释

幀率：Frames Per Second，人眼，一般每秒 10\-12 帧，基本上就是'动画'，但也仅仅是低要求，20\-30 是比较舒适的。50\-60 基本上极限，也是效果最好的。前提是：帧率是均速且象素是固定的。

渲染帧：是由 GPU 控制速度，也是由 GPU 渲染出来的

逻辑幀率：S 端定义，每秒发送多少逻辑帧

逻辑帧：由玩家操作的一系列指令的集合，打包，由 C 端发送给 S 端，S 端接收后再下发给所有 C 端，C 端接到逻辑帖后，拆分，开始执行并渲染

缓冲帧：由玩家操作的一系列指令的集合，打包，C 端发送给 S 端，在固定时间内没

有接收到 S 端指令，缓存下来的帧

播放帧：C 端接收到逻辑帧后，通知执行，即渲染此帧的所有指令

## 技术点选型：

网络传输协议：websocket
传输数据类型：json protobuf
前端：cocos/js
后端：golang

## 原因

websocket ：现有协议，随便找个类库直接可以用，且支持网页，相比于直接用 TCP/UDP 更简单，减少很多问题，像：开发周期、人员级别、粘包 等等

json & protobuf:前期先使用 json,保证最快实现 demo，后期上 protobuf 做优化，但最好的情况，还是自己写个内容协议体，减少代码处理时间

## 开发模块划分

1. 协议及内容规则制定
2. 规则配置定义及接口
3. 文档及流程图
4. 匹配
5. 房间
6. WS 协议连接过程
7. PIN/PONG 心跳
8. 帧同步\-准备/匹配/取消匹配/
9. 帧同步\-初始化战场、开始游戏
10. 帧同步\-断线/重连
11. 帧同步\-历史播放
12. 帧同步\-游戏结束、打扫战场
13. protobuf
14. metrics
15. 前端 JS\-DEMO
16. 联调

## api 数据通信

先看一个 demo:

```
1000{"token":"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVaWQiOjEsIkV4cGlyZSI6MTYxNzMyOTk2MywiQVRpbWUiOjE2MTcyNDM1NjMsIkFwcElkIjoxfQ.XxbHWdSZx4WvXklXHfCWjSpJAIZo2aVv4aUuD3SNsIE"}
```

数据内容包含 2 个部分：

1. 动作 ID:4 个字符，数字类型，主要用于标识该次请求的动作是什么
2. JSON 对象里才是一次通信中的具体内容

> ps:传输的时候千万别忘了前面的动作 ID 号

另外，如果后期使用 TPC，牵扯到粘包的话，协议的内容体还得再修改，头部要增加 1 个字节：长度，尾部要加上分隔符

## 具体 API 列表详情：

这里就不列举了，太占地方，下面有个连接地址，[直接点击吧](http://39.106.65.76:3333/www/apilist.html)

C 端读取基础配置信息：

> http://39.106.65.76:3333/www/getServer

ps:所有配置均存于 S 端，动态获取，C 端的包里尽量不要存这些东西。

```
{
    "Host":"127.0.0.1",
    "Port":"2222",
    "MapSize":10,
    "RoomPeople":4,
    "Uri":"/ws",
    "OffLineWaitTime":20,
    "LoginAuthType":"jwt"
}

```

## 如何定义\<断开连接/玩家下线\>

1. C 端主动断开连接
2. S 端主动断开连接
3. C 端，4G 切 5G，4/5G 切 WIFI，
4. C 端，直接关闭网页、直接 KILL 游戏进程
5. 玩家锁屏
6. 玩家切换到其它进程
7. 手机内存不够，进程被 OS KILL 了
8. S 端在 N 秒内没有收到 C 端连接的任何动作
9. RTT 过大，也要当成连接断开

> 挺操蛋的一个东西，比较复杂，而且可能大概率是：大部分的情况 S 端都收不到 C 端的正常关闭

## RTT 计算

1. 每次请求，记录一下 RTT 值
2. 要有记录表，记录每次的 RTT 值，然后取中位数
3. 最好还是用官方推荐的加权公式算法
4. 一但过大，就得锁操作，但这个阀值应该如何设置？460？

> 又是一个极其复杂的东西，比上面还要复杂，而且，C/S 两端大部分计算，都得基于这个值，尤其像：RTT 过大即认为用户已断线锁操作，所以，非常重要。

## 录播/快照

S 端应保存一局内，玩家的所有操作

## 断线重连

一但有玩家意外断线后，要支持重连，S 端应保存所有历史记录，供 C 端玩家重启后，恢复游戏进度

## 游戏流畅性

1. S 端其实不多，就是网络收发包正常，且不积压即可
2. C 端这里要求就是比较多了，如下：
   逻辑帧与渲染分离
   插值
   Dead Reckoning
   影子跟随
   延迟补偿
   预测/回滚
   浮点数转定点数
   锁用户操作
   顺序性，如：多协程/线程会有不确定性
   GC/FULL GC ，执行时触发 GC 或者内存不够触发 FULL GC
   .....这里就不一一列举，毕竟后端出身，C 端有点弱....

> PS:另外，可以参考下 ESC 框架，及守望先锋的构架

## 日志

1. C 端在保证性能的前提下，尽量多记日志，尤其像崩溃意外中止连接这种
2. C 端定期上传日志到后端
3. S 端必须得多记、甚至全记，并且保存
4. 提供日志收集工具、日志查看工具

## 开发/测试工具

1. metrics
   房间信息
   当前连接数
   当前协程数
   玩家连接状态
   I/O 字节流
   ...
2. apilist
3. protobuf 文件、protubf 编译工具
4. S 端配置信息
5. api 协议号配置信息
6. 日志收集/查看

详细的两端 C/S 交互流程图,[点我](http://39.106.65.76:3333/www/sync_frame_client_server.jpg)

可以通过浏览器查看所有相关文档

> http://39.106.65.76:3333/www/index.html

# 开发过后的总结

## 代码中{时间} 概念

以前的项目，精确到秒级，基本上就是顶天了。最多就是生成唯一 ID 的时候，偶尔出现重复，加个再加个毫秒即可。而帧同步：对毫秒的理解理深了。像：

1. 每秒同步多少次
2. RTT 每秒测试多少次

区别：以前就是被动加个时间而以，现在是要你确定在某些毫秒内就必须得干些时间，对毫秒这个概念必须理解深刻。到底什么是毫秒？什么是秒？到底是什么时间周期？每个周期具体能干什么事儿？一个周期没有干完的事儿，如何处理？

> 真的是一秒内能干太多的事情，一个 CPU 的执行时间片，好厉害....

## 技术选型

选型的工作，对项目的方方面面影响都太大了，以前的选型都挺鸡肋，你咋选其实到后期都没特别大的影响。
而帧步的选型点还不少，且影响都挺大，像

1. 协议
2. 传输内容的类型
3. 囚徒/乐观/预测回滚

## 多协程/多进程

这个是最复杂的，虽然宏观看就那么几个功能，但为了保持稳定性，一个主协程，要配合太多的辅助协和来帮忙'擦屁股'
所有多协程/多进程的要支持：

1. 统一建立
2. 统一删除
3. 某一个的建立
4. 某一个的删除

从这点上看，还是多协程简单些，尤其 GO 的多协和太方便了。不过性能可能不如 C++ 的多线程

## 延迟

其实上面对时间没太深的理解，这里对延迟的理解也肯定也好不到哪里去。

以前对延迟的处理也没太多的办法，或者说，最多只能在两端代码级别的优化。而实际真的延迟是：

1. 对网络要敏感，敏感到毫秒级。
2. 解决方法不能光光在两端写些简单的应用层代码，要不定期加 TIMER
3. 制定的延迟容错方案，必须得：动态可伸缩，且必须得细致到某个 FD 的层面

## 垃圾回收

还是 PHP 用惯了，有他的 ZEND 虚拟机，帮你做了大部分优化，你只要 PHP 的代码不写的特别夸张，基本上服务器没啥感觉。但是 GO 虽然也有内核功能，但是同样的程序启动，GO 吃服务器的资源是挺可怕的。

## GO 协程

起初写 GO，还是有点基于多进程/多线程模型，可能是像 apache nginx php mysql 这些都是 C 语言编写，脑子里还是 IPC 通信那套理论。对网上的 GO 协程没太多的概念。等帧同步写到后期的时候，发现两者的模型真的是相差挺大的。

GO 里的 chan 这个东西，真的是太方便的。他并不是传统上的 IPC 通信，并不强调进程/协程 或者跟 OS 想关的概念，他的通信就是：我这个协程想要 IO，那就把这个管道发给另外一个协程。非常 简单，一个 chan 就解决了。

有时候，写着写代码，真是感叹万幸这是选择了 GO 的协程模式，开发起来确实简单，这要是 C\+\+估计得玩死我

但也有缺点：程序里到处都是协程，且很多协程就是些些死循环\+等待信号，管理起来挺复杂。且吃资源

## 工业级的软件

还是 LAMP/NAMP 这些开源软件用多了，偶尔能用到一些底层的东西：像信号捕捉、metrics 等等，但真的让你完全的用一门静态语言开发一个 伪完整的后端服务型的软件，好多点，还是有点难，像：metrics 的设计 ，不是使用 3 方开源的，而是纯自己写... 各种测试工具等等

## 最后

帧同步，确实扒了我一层皮，方方面面的~

其根本原因就是：

1. 没真的动手搞过，从 0 开始搞且不用 3 方软件
2. 你要根据需求/论文 ，亲自动手实现一个略完整的工业极的软件
3. 没有完整的产品需求，也没有完整的文档，只有一点点扒
4. 对底层的理解还不够，如：毫秒、时间周期、RTT、TCP 拥塞等

像在鲸鱼的时候，虽然读些外国的英文文档费劲儿，写报告也是麻烦，测试也是麻烦，但终归还是要用别人开发好的软件。而这，不光光是调研，还得把文档落实成最后的项目，TMD，很多细节，文档压根没有，你自己还得给实现了...就算文档上有，你也未必真的能 100%理解，等真的动手实现，才能理解...

哎~~~ 像：各种临界值，RTT 计算公式 等等吧。联调也是麻烦的要死，即使我自己写了个前端 DEMO，依然比之前的模式工都复杂。
