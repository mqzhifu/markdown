# 概览

zookeeper atomic broadcast:zookeeper 原子广播协议

> fast paxos 协议的简化版本

最终一致性
强/单 leader 模式
leader-follower 写交互：2 次(2 阶段提交的模式)
client-leader 写交互：1 次

# 单 leader 优点

1. 原子操作：递增、写操作
2. 原子广播：由一台机器向其它机器广播
3. 消息的顺序性(队列)

# 使用 zk 的中间件

dubbo:注册中心 配置中心
spring cloud 注册中心
solr 管理集群
kafka 管理集群
curator 分布式锁
tinyld 分布式 ID
seata 配置中心
大数据库领域

# 内部结构

1. dataTree-->真实的数据，在内存中。读写最终的数据是它。
2. 投票箱-->好像是个数组，\[ \( 2 , 2, 100 \) \] ,\[ \( 投票者 myid, 投给谁 myid, 本机 zxid \) \]
3. 日志文件--> zxid ,事务 ID，自增连续 ID
   1. 高 32 位-epoch -->任期
   2. 低 32 位-counter -->计算数
4. myid -->配置文件中写死的
5. 队列，广播发送消息的时候，内部维持一个队列

每台 zk 节点，都会有一个日志文件
每来一个请求，会记录到日志文件中，当时 zxid\+1
> zxid，如：100000001 ,100000002......200000001,200000002

# 角色

leader 通过选举，在集群中选出一台节点当 leader，失败重启后，可参加选举
follower 跟随 leader 的脚步，用户的读操作直接在本节点操作\(也可设置 sync 参数\)，写操作则需要转到 leader 执行，可参加选举
observer 观察者/备用机，依然能提供读操作直接在本节点操作，写操作则需要转到 leader 执行，不参考选举，不参加第一阶段预准备

# 启动/初始化

1. 读取 FileTxnSnapLog
2. 构建 ZKDatabase
3. PlayBackListener
4. 将硬盘里的日志文件恢复到内存中,Zookeeper 会加载最新的至多 100 个快照文件
5. 检查 ZXID epoch

# 启动/数据同步

1. 好像所有服务器启动后，都会开启一个 Learner 进程
2. leader 选举成功后，每个 follower 的 Learner 进程会向 leader 进行注册
3. 当所有正常的机器注册成功 Learner 后，开始数据同步
4. Learner 服务器会发送给 Leader 服务器一个 Ack_Epoch_lastZxid 数据包
5. Leader 接收，并解出 EPOCH
6. 通过两端的 epoch\+zxiid，做出下面 4 种处理：
   1. 直接差异化同步\(diff 同步\)，简单说：对端的日志完全落后于我，把 leader 上的日志整理下打包发给对端，让它补全即可
   2. 先回滚再差异化同步，对端的机器上有未提交的日志。如：旧 leader 日志已经写入等待 ACK 的时候挂了，另外机器重新选举成功，并正常又接收新的消息了，旧 leader 启动后，那么就会出现：旧 leader 未提交的那条日志，新 leader 中没有，leader 找出具体位置后，让那条未提交的数据回滚/删除，然后再开始对落后的数据进行同步
   3. 回滚同步，发现 Learner 的 lastZxid 比自己的 maxCommittedLog 还大，那直接让 Learner 回滚到 maxCommittedLog 值即可。这种情况：旧 leader 有一条数据未同步就挂了，另外 3 台机器重选 ，可能有一台日志比较落后，但是也成功当选了新 leader，那就以新 leader 为最准的，把旧 leader 未同步的数据直接删除
   4. 全恢复，如果 Learner 的 lastZxid 比自己的 minCommittedLog 还小，直接把新 leader 覆盖过去即可

> 启动、初始化、数据同步/修复阶段，肯定是无法对外部提供服务的，选举期间也是不能的，必须保证各节点的数据全部正确后，才可以。感觉 ZK 还是挺保守的。

# 选举

1. 数据恢复
2. 每台机器启动后，会找 leader，或被 leader 找
3. 如果找不到 leader 或也没有 leader 找自己
4. 如果自己不是 observer 角色
5. 开启选举，从配置配置文件中找到其它机器的 ip\+port
6. 与其它机器建立 socket 连接
7. 如果在这期间收到消息：当前集群有 leader，你可能只是与它短暂未连接上，先退回到 follower 状态
8. 第一次投票，默认肯定是要投自己的
9. 将 zxid \+ myid 打包，广播给其它的机器
10. 更新本机的投票箱，将自己的选票存进去
11. 如果接收到其它机器的选票
    1. zxid 比自己的小，给自己投票，并存到箱子中\(不重复投\)，然后广播给其它 follower
    2. zxid 相等，再比较 myid
       1. myid 比自己大，将自己选自己的选举删除，将对方的选票保存，同时自己再投对方一票，保存本地，再广播该投票
       2. myid 比自己的小，给自己投票，并存到箱子中\(不重复投\)，然后广播给其它 follower
    3. zxid 比自己大，将之前的选票\(自己选自己的\)删除，将对方的选票保存，同时自己再投对方一票，保存本地，再广播该投票

> 每台机器来来回回：会收到多张选票，同时触发自己的被动机制，再发出新的选票\(比对 zxid\+myid\)。最终每个节点的选票箱其实结果是一样的
> 比对 zxid\+myid 过程中，发现对方更大就修改自己的投票，这个算是：选举谦让制度，非抢占式

1. 各节点不停的统计选票，只有过半，即认为自己是 leader
2. 新 leader，同步/广播给其它选举节点，告知自己是 leader
3. 其它机器接收到 leader 发过来的广播消息，自己将角色切换成 follower

> 两台机器交换选票肯定是通过网络 socket，两台机器如果一起启动，互相建立 socket 连接，如果是 myid 小的连 myid 大的，会被 myid 大的给拒掉
> 选举过程中：肯定是无法处理 client 请求的。

如果：现在集群是 5 台机器

1. 如果先启动 1 2 ，另外 3 台不启动，能选出 leader 么？肯定不能，两台机器的选票是不可能过半的
2. 如果全部正常，leader 为 2，此时 3 4 5 挂了，还能提供服务么？肯定不能，因为消息的 ack 永远不可能过半。leader 发现一半挂了，它会由 leader 转换成 follower，重新选举

# 读写操作

所有角色均能接受 client 读写操作，读操作在本地均可处理，写操作 follower 和 observer 要转交给 leader 处理

leader 接收写请求：

1. 判断前一个预提案是否生效（过半节点回复了上条消息的 ack）
   1. 如果未生效，阻塞，直到上一个提案 OK 了
   2. 如果已生效，继续下面

> 这就保证了时序性/顺序性

1. 生成日志数据，zxid\+1
2. 将日志持久化
3. 预提交
4. 阻塞，等待 follower ack
5. 提交,更新 dataTree
6. 将日志异步广播到 follower
7. 将日志异步广播到 observer
8. 返回 client 成功

特殊的情况 1：步骤 4，如果未收到过半 ACK？两种情况：

1. 设一个定时器，超时,回滚，返回 client 失败
2. 或者节点之间有心跳，检查出有一半节点 down 机或一半节点网络错误，开启重新选举\(回滚，返回 client 失败\)
3. leader 挂了\(网络波动，短时间内不能恢复\)，重启/恢复后，该条数据被删除
4. 网络波动，其它节点开启重选，机器马上恢复参与选举，因为 zxid 大，还会继续成为 leader，选不上，该条数据删除

特殊的情况 2：步骤 4，leader 本地已提交，在通知其它 follower 提交时挂了

1. 如果新 leader 是已收到的那条消息情况，当然新 leader，那么数据正常
2. 如果新 leader 是未收到的那条消息情况，当然新 leader，该数据就丢失了

> 因为二阶段提交的核心思想就是，第一阶段来确认其它机器是否这正常，发生上面的错误的概率极低，就算发生了，但好像 raft zab 对这种情况好像都没做明确说明，不知道具体原因。
> 如果 zab 是顺序存储\(阻塞模式\)，最多只能丢失一条，貌似还好。

# observer 及 IO 分析

这里有个问题：机器越多，读操作更快。但是写会慢？因为，需要同步半数的机器，等待 ACK。

observer：

1. leader 最终将已提交的日志，再异步转发给 observer
2. 不参与选举
3. 提高写操作，leader 同步日志，不需要等待 observer 的 ack
4. 增加读操作

> 如果机器变多后，把其角色定义成 observer 角色，可以提高读的速度，同时减少写的时候有过多的 rsync

读写分离后分析：

1. 选举的速度肯定是由 follower 决定\(网络原因先不算,旧 leader 要那么直接挂了不参选，要么已经转变成了 follower\)
2. 读的速度由：follower\+observer 数量决定
3. 写的速度由：leader \+ 同步 follower 数量决定

# 脑裂分析

会有脑裂么？

1. 如果是 6 台，挂了 3 台，就不会有 leader，未过半
2. 如果是 5 台，挂了 2 台，3 台那部分是正常，另外 2 台如果是 leader 会放弃重新选，但永远选不出来 leader

如果机器总数是偶数，会是什么样？如果是 6 台，挂了 3 台，另外 3 台是肯定选不出 leader

所以整体看，还是奇数台机器更优。

> 不过这里要注意：忽略掉 observer

# watch

# 锁

# 数据文件树
